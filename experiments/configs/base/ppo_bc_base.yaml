# Base configuration for PPO with Behavioral Cloning
algorithm: "ppo_bc"
seed: 42

# PPO parameters (inherited from ppo_base)
ppo:
  n_steps: 2048
  batch_size: 64
  n_epochs: 10
  gamma: 0.99
  gae_lambda: 0.95
  clip_range: 0.2
  clip_range_vf: null
  ent_coef: 0.0
  vf_coef: 0.5
  max_grad_norm: 0.5
  target_kl: null

# BC specific parameters
bc:
  bc_loss_weight:
    needle_reach: 0.05
    peg_transfer: 0.02
  demo_batch_size: 32
  use_bc_initialization: true  # Whether to initialize from BC model
  bc_model_path: null  # Will be set if using BC initialization

# Network architecture
network:
  policy: "MlpPolicy"
  policy_kwargs:
    net_arch: [256, 256]
    activation_fn: "relu"

# Training parameters
training:
  needle_reach:
    total_timesteps: 100000
    learning_rate: 3.0e-4
  peg_transfer:
    total_timesteps: 300000
    learning_rate: 3.0e-4

# Data parameters
data:
  num_demonstrations: 100
  demo_path: null  # Will be set based on task

# Environment parameters
env:
  n_envs: 4
  normalize: false
  reward_scale: 1.0

# Logging parameters
logging:
  tensorboard: true
  csv_logging: true
  log_interval: 10
  save_freq: 10000
  eval_freq: 5000
  eval_episodes: 10
  verbose: 1