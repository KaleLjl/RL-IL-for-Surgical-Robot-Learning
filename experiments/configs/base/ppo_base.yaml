# Base configuration for Proximal Policy Optimization
algorithm: "ppo"
seed: 42

# PPO specific parameters
ppo:
  n_steps: 2048
  batch_size: 64
  n_epochs: 10
  gamma: 0.99
  gae_lambda: 0.95
  clip_range: 0.2
  clip_range_vf: null
  ent_coef: 0.0
  vf_coef: 0.5
  max_grad_norm: 0.5
  target_kl: null

# Network architecture
network:
  policy: "MlpPolicy"
  policy_kwargs:
    net_arch: [256, 256]
    activation_fn: "relu"

# Training parameters
training:
  needle_reach:
    total_timesteps: 100000
    learning_rate: 3.0e-4
  peg_transfer:
    total_timesteps: 300000
    learning_rate: 3.0e-4

# Environment parameters
env:
  n_envs: 4  # Number of parallel environments
  normalize: false
  reward_scale: 1.0

# Logging parameters
logging:
  tensorboard: true
  csv_logging: true
  log_interval: 10
  save_freq: 10000
  eval_freq: 5000
  eval_episodes: 10
  verbose: 1